# LLM Basics â€” Large Language Models

> Learn how LLMs work, from architecture to fine-tuning

---

## ğŸ“ Structure

```
LLM(Basics)/
â”œâ”€â”€ Architecture/        # Build GPT from scratch
â”œâ”€â”€ PRE-TRAINING/        # Train your own model
â”œâ”€â”€ WEIGHT-LOADING/      # Load pre-trained weights
â””â”€â”€ FINE-TUNING/         # Fine-tune for tasks
```

---

## ğŸ—‚ï¸ Modules

### 1. Architecture
Learn the transformer architecture that powers GPT models.

**Files:**
- `main.py` â€” GPT model implementation
- `supplementary.py` â€” Components (Attention, LayerNorm, etc.)

**Key Concepts:**
- Token embeddings
- Position embeddings
- Multi-head attention
- Feed-forward networks
- Layer normalization

**Run:**
```bash
cd Architecture
python main.py
```

---

### 2. Pre-Training
Train a GPT model from scratch.

**Files:**
- `main.py` â€” Training loop
- `supplementary.py` â€” Data loading & utilities
- `RunOnPreTrainedData.py` â€” Test trained model
- `RunInNamedFile.py` â€” Load and run

**Key Concepts:**
- Data preparation
- Loss calculation
- Gradient descent
- Model checkpointing

**Run:**
```bash
cd PRE-TRAINING
python main.py  # Train model
python RunOnPreTrainedData.py  # Test it
```

---

### 3. Weight Loading
Load pre-trained GPT-2 weights into your model.

**Files:**
- `main.py` â€” Load and generate text
- `gpt_download.py` â€” Download GPT-2 weights
- `supplementary.py` â€” Model components
- `run.py` â€” Quick test

**Key Concepts:**
- Weight transfer
- Model compatibility
- Inference

**Run:**
```bash
cd WEIGHT-LOADING
python main.py
```

---

### 4. Fine-Tuning
Fine-tune a pre-trained model for specific tasks.

**Files:**
- `main.py` â€” Fine-tuning with LitGPT
- `evaluation.py` â€” Evaluate performance

**Key Concepts:**
- LoRA (Low-Rank Adaptation)
- Instruction tuning
- Evaluation metrics

**Run:**
```bash
cd FINE-TUNING
python main.py
```

---

## ğŸ› ï¸ Requirements

```bash
pip install torch tiktoken matplotlib
pip install litgpt  # For fine-tuning
```

---

## ğŸ“š Resources

| Topic | Resource |
|-------|----------|
| Transformers | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) |
| GPT | [OpenAI GPT](https://openai.com/research) |
| PyTorch | [pytorch.org](https://pytorch.org/) |
| LitGPT | [GitHub](https://github.com/Lightning-AI/litgpt) |

---

## ğŸ’¡ Tips

1. **Start small** â€” Use smaller models first (124M)
2. **Understand architecture** â€” Read `supplementary.py` carefully
3. **Experiment** â€” Change hyperparameters
4. **Monitor loss** â€” Watch training curves

---

Happy learning! ğŸ¤–
